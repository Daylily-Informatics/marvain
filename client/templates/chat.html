<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chat — marvain</title>
  <style>
    body { font-family: system-ui, -apple-system, Arial, sans-serif; margin: 24px; line-height: 1.35; }
    #conversation { border: 1px solid #ddd; border-radius: 10px; padding: 12px; height: 55vh; overflow-y: auto; background: #fff; }
    .msg { margin: 8px 0; padding: 8px 10px; border-radius: 10px; max-width: 72ch; }
    .user { border: 1px solid #cfd; }
    .agent { border: 1px solid #ddf; }
    .meta { color: #666; font-size: 0.92em; }
    #input-area { display:flex; gap: 8px; margin-top: 10px; align-items: center; flex-wrap: wrap; }
    input[type="text"] { padding: 9px; border-radius: 10px; border: 1px solid #bbb; min-width: 320px; flex: 1; }
    button { padding: 9px 10px; border-radius: 10px; border: 1px solid #bbb; background: #fff; cursor:pointer; }
    button:hover { border-color: #888; }
    .row { display:flex; gap: 10px; flex-wrap: wrap; align-items: center; }
    select { padding: 6px; border-radius: 8px; border: 1px solid #bbb; max-width: 360px; }
    .pill { border: 1px solid #ddd; border-radius: 999px; padding: 4px 9px; }
    .card { border: 1px solid #ddd; border-radius: 10px; padding: 12px; margin-top: 14px; }
    .debug-log { max-height: 180px; overflow-y: auto; background: #f7f7f7; border: 1px solid #eee; padding: 8px; border-radius: 8px; }
    .debug-tools { display: grid; gap: 10px; margin-top: 10px; }
    .debug-tool { border: 1px dashed #ccc; padding: 8px; border-radius: 8px; }
    .debug-tool-title { font-weight: 600; }
    .debug-output { background: #0b1021; color: #e3e3e3; padding: 10px; border-radius: 8px; white-space: pre-wrap; max-height: 260px; overflow-y: auto; }
  </style>
</head>
<body>
  <h1>Chat</h1>
  <div class="meta">
    Connected to: <strong>{{ stack_name }}</strong>
    (<code>{{ state.selected_endpoint }}</code>)
    <div>Session: <strong>{{ state.selected_session or "Auto-generated" }}</strong>{% if not state.selected_session %} (new ID will be generated as you chat){% endif %}</div>
  </div>

  <div class="row" style="margin-top: 10px;">
    <label class="pill">
      ASR mode:
      <select id="asrMode">
        <option value="server" selected>Server (AWS Transcribe)</option>
        <option value="browser">Browser (Web Speech API fallback)</option>
      </select>
    </label>
    <label class="pill">
      Language:
      <select id="languageCode">
        <option value="en-US" selected>en-US</option>
        <option value="en-GB">en-GB</option>
        <option value="es-US">es-US</option>
        <option value="fr-FR">fr-FR</option>
      </select>
    </label>
    <button id="enableAudioBtn">Enable audio</button>
    <span class="pill" id="asrStatus">ASR: idle</span>
    <span class="pill" id="partialPill" style="display:none;"></span>
  </div>

  <div class="row" style="margin-top: 10px;">
    <span class="pill">Push-to-talk: <button id="pttBtn">Hold to talk</button></span>
    <label class="pill">
      <input type="checkbox" id="ambientToggle" /> Ambient listening
    </label>
    <span class="pill meta">Output device selection needs Chrome/Edge + HTTPS/localhost.</span>
  </div>

  <div class="row" style="margin-top: 10px;">
    <div>
      <div class="meta">Audio input device</div>
      <select id="audioIn"></select>
    </div>
    <div>
      <div class="meta">Audio output device</div>
      <select id="audioOut"></select>
    </div>
    <div>
      <div class="meta">Video input device (display only)</div>
      <select id="videoIn"></select>
    </div>
  </div>

  <div id="conversation" style="margin-top: 12px;"></div>

  <div id="input-area">
    <input type="text" id="user_input" placeholder="Type your message"
           onkeydown="if(event.key==='Enter'){ sendMessage(); }" />
    <button onclick="sendMessage()">Send</button>
  </div>

  <div class="row" style="margin-top: 10px;">
    <div class="meta">Optional extra personality prompt (sent with each message)</div>
    <input type="text" id="persona_input" placeholder="e.g. be terse, ask clarifying questions, etc." style="min-width: 520px;" />
  </div>

  <div class="row" style="margin-top: 10px;">
    <div class="meta">Optional speaker name (helps voice registry)</div>
    <input type="text" id="speaker_name_input" placeholder="e.g. Major" style="min-width: 260px;" />
  </div>

  <div class="card" id="debugCard">
    <h2 style="margin-top:0">Debug</h2>
    <div class="meta">Verbose logging is {{ 'enabled' if state.verbose else 'disabled' }} (toggle in <a href="/settings">Settings</a>).</div>
    <div class="meta">Use the tools below to run debug helpers and view their output directly in the GUI.</div>
    <div id="verboseLogs" class="debug-log"></div>
    <div class="debug-tools" id="debugTools"></div>
    <pre id="debugOutput" class="debug-output"></pre>
  </div>

  <p style="margin-top: 12px;"><a href="/">← Back</a></p>

<script>
  const verboseEnabled = {{ 'true' if state.verbose else 'false' }};
  const convo = document.getElementById('conversation');

  const asrModeSel = document.getElementById('asrMode');
  const languageSel = document.getElementById('languageCode');
  const enableAudioBtn = document.getElementById('enableAudioBtn');
  const pttBtn = document.getElementById('pttBtn');
  const ambientToggle = document.getElementById('ambientToggle');
  const asrStatus = document.getElementById('asrStatus');
  const partialPill = document.getElementById('partialPill');

  const audioInSel = document.getElementById('audioIn');
  const audioOutSel = document.getElementById('audioOut');
  const videoInSel = document.getElementById('videoIn');
  const debugLogBox = document.getElementById('verboseLogs');
  const debugOutput = document.getElementById('debugOutput');
  const debugToolsContainer = document.getElementById('debugTools');

  let selectedOutputDeviceId = null;
  let selectedMicDeviceId = null;

  function appendMsg(role, text) {
    const div = document.createElement('div');
    div.className = 'msg ' + role;
    div.textContent = text;
    convo.appendChild(div);
    convo.scrollTop = convo.scrollHeight;
  }

  function appendMeta(text) {
    const div = document.createElement('div');
    div.className = 'meta';
    div.textContent = text;
    convo.appendChild(div);
    convo.scrollTop = convo.scrollHeight;
  }

  function appendDebugLog(text) {
    if (!debugLogBox) return;
    const div = document.createElement('div');
    div.textContent = text;
    debugLogBox.appendChild(div);
    debugLogBox.scrollTop = debugLogBox.scrollHeight;
  }

  function setDebugOutput(text) {
    if (!debugOutput) return;
    debugOutput.textContent = text || '';
  }

  function setAsrStatus(t) {
    asrStatus.textContent = 'ASR: ' + t;
  }

  function setPartial(text) {
    if (!text) {
      partialPill.style.display = 'none';
      partialPill.textContent = '';
      return;
    }
    partialPill.style.display = 'inline-block';
    partialPill.textContent = 'partial: ' + text;
  }

  // Downsample Float32 audio buffer to 16kHz for AWS Transcribe.
  // Uses a simple averaging approach (good enough for speech).
  function downsampleBuffer(buffer, inputSampleRate, outputSampleRate) {
    if (outputSampleRate === inputSampleRate) {
      return buffer;
    }
    const sampleRateRatio = inputSampleRate / outputSampleRate;
    const newLength = Math.round(buffer.length / sampleRateRatio);
    const result = new Float32Array(newLength);
    let offsetResult = 0;
    let offsetBuffer = 0;
    while (offsetResult < result.length) {
      const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);
      // average the samples in the old buffer between these two points
      let accum = 0;
      let count = 0;
      for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
        accum += buffer[i];
        count++;
      }
      result[offsetResult] = count > 0 ? (accum / count) : 0;
      offsetResult++;
      offsetBuffer = nextOffsetBuffer;
    }
    return result;
  }

  async function playAgentAudio(audioObj) {
    try {
      const audio = new Audio();
      if (audioObj.url) audio.src = audioObj.url;
      else if (audioObj.data && audioObj.content_type) audio.src = 'data:' + audioObj.content_type + ';base64,' + audioObj.data;
      else return;

      // Route playback to selected output device when supported.
      if (selectedOutputDeviceId && typeof audio.setSinkId === 'function') {
        try { await audio.setSinkId(selectedOutputDeviceId); } catch(e) { /* ignore */ }
      }
      await audio.play();
    } catch (e) {
      appendMeta('audio playback failed: ' + e);
    }
  }

  async function sendToAgent(messageText) {
    const persona = (document.getElementById('persona_input').value || '').trim();
    try {
      const response = await fetch('/api/send_message', {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify({ text: messageText, personality_prompt: persona })
      });
      const data = await response.json();
      if (!response.ok || data.error) {
        appendMsg('agent', '[Error] ' + (data.error || ('HTTP ' + response.status)));
        return;
      }
      if (data.reply_text) appendMsg('agent', data.reply_text);
      if (data.actions && data.actions.length) appendMeta('actions: ' + JSON.stringify(data.actions));
      if (data.verbose_logs) data.verbose_logs.forEach((log) => { appendDebugLog(log); if (verboseEnabled) appendMeta('[verbose] ' + log); });
      if (data.audio) await playAgentAudio(data.audio);
    } catch (e) {
      appendMsg('agent', '[Error] Agent not reachable');
    }
  }

  async function sendMessage() {
    const input = document.getElementById('user_input');
    const text = (input.value || '').trim();
    if (!text) return;
    appendMsg('user', text);
    input.value = '';
    await sendToAgent(text);
  }

  // --- Device enumeration + selection ---
  function addOpt(sel, label, value) {
    const o = document.createElement('option');
    o.value = value;
    o.textContent = label;
    sel.appendChild(o);
  }

  async function refreshDevices() {
    const devices = await navigator.mediaDevices.enumerateDevices();

    const prevMic = audioInSel.value;
    const prevOut = audioOutSel.value;
    const prevVid = videoInSel.value;

    audioInSel.innerHTML = '';
    audioOutSel.innerHTML = '';
    videoInSel.innerHTML = '';

    const mics = devices.filter(d => d.kind === 'audioinput');
    const outs = devices.filter(d => d.kind === 'audiooutput');
    const cams = devices.filter(d => d.kind === 'videoinput');

    mics.forEach((d, i) => addOpt(audioInSel, d.label || ('mic ' + (i+1)), d.deviceId));
    outs.forEach((d, i) => addOpt(audioOutSel, d.label || ('speaker ' + (i+1)), d.deviceId));
    cams.forEach((d, i) => addOpt(videoInSel, d.label || ('camera ' + (i+1)), d.deviceId));

    if (prevMic) audioInSel.value = prevMic;
    if (prevOut) audioOutSel.value = prevOut;
    if (prevVid) videoInSel.value = prevVid;

    selectedMicDeviceId = audioInSel.value || null;
    selectedOutputDeviceId = audioOutSel.value || null;
  }

  enableAudioBtn.addEventListener('click', async () => {
    try {
      // Request permissions so device labels are available.
      const constraints = { audio: true, video: false };
      const s = await navigator.mediaDevices.getUserMedia(constraints);
      s.getTracks().forEach(t => t.stop());
      await refreshDevices();
      appendMeta('audio permissions granted');
    } catch (e) {
      appendMeta('audio permission failed: ' + e);
    }
  });

  audioInSel.addEventListener('change', () => {
    selectedMicDeviceId = audioInSel.value || null;
    // Restart capture with new device if running.
    if (serverAsr.captureActive) {
      serverAsr.stopCapture();
      // If ambient is enabled, restart capture.
      if (ambientToggle.checked && asrModeSel.value === 'server') {
        serverAsr.startCapture().catch(e => appendMeta('capture restart failed: ' + e));
      }
    }
  });

  audioOutSel.addEventListener('change', () => {
    selectedOutputDeviceId = audioOutSel.value || null;
  });

  // --- Server-side ASR (AWS Transcribe streaming via FastAPI websocket) ---
  const serverAsr = {
    ws: null,
    wsCanSend: false,
    sessionActive: false,
    captureActive: false,
    audioCtx: null,
    processor: null,
    source: null,
    stream: null,
    gain: null,
    preRoll: [],
    preRollMax: 6,
    lastVoiceMs: 0,
    vadThreshold: 0.020,
    silenceMs: 900,

    async startCapture() {
      if (this.captureActive) return;
      const deviceId = selectedMicDeviceId;
      const constraints = {
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          ...(deviceId ? { deviceId: { exact: deviceId } } : {})
        },
        video: false
      };

      this.stream = await navigator.mediaDevices.getUserMedia(constraints);
      const AC = window.AudioContext || window.webkitAudioContext;
      // Ask for 16k; browser will resample if needed.
      this.audioCtx = new AC({ sampleRate: 16000 });
      this.source = this.audioCtx.createMediaStreamSource(this.stream);

      // ScriptProcessor is widely supported; buffer size 2048 keeps latency reasonable.
      this.processor = this.audioCtx.createScriptProcessor(2048, 1, 1);
      this.gain = this.audioCtx.createGain();
      this.gain.gain.value = 0; // prevent audible loopback

      this.source.connect(this.processor);
      this.processor.connect(this.gain);
      this.gain.connect(this.audioCtx.destination);

      this.processor.onaudioprocess = (e) => {
        const input = e.inputBuffer.getChannelData(0);
        if (!input) return;

        // Ensure we feed AWS Transcribe 16kHz PCM.
        const inRate = (this.audioCtx && this.audioCtx.sampleRate) ? this.audioCtx.sampleRate : 16000;
        const buf = (inRate === 16000) ? input : downsampleBuffer(input, inRate, 16000);

        // Compute RMS for simple VAD.
        let sum = 0;
        for (let i = 0; i < buf.length; i++) sum += buf[i] * buf[i];
        const rms = Math.sqrt(sum / buf.length);
        const now = performance.now();

        // Convert Float32 [-1,1] to Int16 PCM LE.
        const ab = new ArrayBuffer(buf.length * 2);
        const dv = new DataView(ab);
        for (let i = 0; i < buf.length; i++) {
          let s = Math.max(-1, Math.min(1, buf[i]));
          dv.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
        }

        const ambient = ambientToggle.checked && asrModeSel.value === 'server';

        if (ambient) {
          if (rms > this.vadThreshold) {
            this.lastVoiceMs = now;
            if (!this.sessionActive) {
              this.startSession('ambient').catch(err => appendMeta('ASR start failed: ' + err));
            }
          }

          if (this.sessionActive && this.wsCanSend && this.ws && this.ws.readyState === 1) {
            if (this.ws.bufferedAmount < 2_000_000) this.ws.send(ab);
          } else {
            // keep a short pre-roll so we don't clip the start
            this.preRoll.push(ab);
            if (this.preRoll.length > this.preRollMax) this.preRoll.shift();
          }

          if (this.sessionActive && this.lastVoiceMs && (now - this.lastVoiceMs) > this.silenceMs) {
            this.stopSession();
          }
        } else {
          // push-to-talk mode: send only while sessionActive
          if (this.sessionActive && this.wsCanSend && this.ws && this.ws.readyState === 1) {
            if (this.ws.bufferedAmount < 2_000_000) this.ws.send(ab);
          } else {
            this.preRoll.push(ab);
            if (this.preRoll.length > this.preRollMax) this.preRoll.shift();
          }
        }
      };

      this.captureActive = true;
      appendMeta('capture started' + (deviceId ? (' (mic: ' + deviceId.slice(0,8) + '…)') : ''));
    },

    stopCapture() {
      try { if (this.processor) this.processor.disconnect(); } catch(e) {}
      try { if (this.source) this.source.disconnect(); } catch(e) {}
      try { if (this.gain) this.gain.disconnect(); } catch(e) {}
      try { if (this.audioCtx) this.audioCtx.close(); } catch(e) {}
      try {
        if (this.stream) this.stream.getTracks().forEach(t => t.stop());
      } catch(e) {}
      this.audioCtx = null;
      this.processor = null;
      this.source = null;
      this.stream = null;
      this.gain = null;
      this.captureActive = false;
      this.preRoll = [];
    },

    _wsUrl() {
      const proto = (location.protocol === 'https:') ? 'wss' : 'ws';
      return proto + '://' + location.host + '/ws/asr';
    },

    async startSession(reason) {
      if (this.sessionActive) return;

      await this.startCapture();

      setAsrStatus('connecting…');
      setPartial('');

      const ws = new WebSocket(this._wsUrl());
      ws.binaryType = 'arraybuffer';
      this.ws = ws;
      this.sessionActive = true;
      this.wsCanSend = false;

      ws.onmessage = (evt) => {
        let msg = null;
        try { msg = JSON.parse(evt.data); } catch(e) { return; }
        if (msg.type === 'asr_ready') {
          setAsrStatus('listening (server)');
        } else if (msg.type === 'partial') {
          setPartial(msg.text || '');
        } else if (msg.type === 'final_chunk') {
          // optional debug
        } else if (msg.type === 'final') {
          setPartial('');
          if (msg.text) appendMsg('user', msg.text);
          setAsrStatus('processing…');
        } else if (msg.type === 'agent_reply') {
          const data = (msg.payload || {});
          if (data.error) appendMeta('agent error: ' + data.error);
          if (data.reply_text) appendMsg('agent', data.reply_text);
          if (data.actions && data.actions.length) appendMeta('actions: ' + JSON.stringify(data.actions));
          if (data.audio) playAgentAudio(data.audio);
        } else if (msg.type === 'error') {
          appendMeta('ASR error: ' + (msg.error || 'unknown'));
        }
      };

      ws.onclose = () => {
        this.ws = null;
        this.wsCanSend = false;
        this.sessionActive = false;
        setAsrStatus('idle');
        // If we're not in ambient mode, release the mic after each utterance.
        const ambient = ambientToggle.checked && asrModeSel.value === 'server';
        if (!ambient) {
          this.stopCapture();
        }
      };

      // Wait for connection open
      await new Promise((resolve, reject) => {
        ws.onopen = () => resolve();
        ws.onerror = (e) => reject(e);
      });

      const persona = (document.getElementById('persona_input').value || '').trim();
      const speakerName = (document.getElementById('speaker_name_input').value || '').trim();

      ws.send(JSON.stringify({
        type: 'start',
        language_code: (languageSel.value || 'en-US'),
        sample_rate_hz: 16000,
        session_id: 'ui-session',
        voice_id: selectedMicDeviceId || null,
        speaker_name: speakerName || null,
        personality_prompt: persona || null,
        reason: reason || ''
      }));

      // send pre-roll first
      this.wsCanSend = true;
      for (const chunk of this.preRoll) {
        if (ws.readyState === 1) ws.send(chunk);
      }
      this.preRoll = [];
      setAsrStatus('listening (server)');
    },

    stopSession() {
      if (!this.sessionActive || !this.ws) return;
      this.wsCanSend = false;
      try { this.ws.send(JSON.stringify({ type: 'stop' })); } catch(e) {}
      setAsrStatus('finalizing…');
    }
  };

  // --- Browser ASR fallback (Web Speech API) ---
  let recognition = null;
  let recognizing = false;
  if ('webkitSpeechRecognition' in window) {
    recognition = new webkitSpeechRecognition();
    recognition.continuous = false;
    recognition.interimResults = false;
    recognition.lang = 'en-US';

    recognition.onresult = function(event) {
      if (event.results && event.results.length > 0) {
        const transcript = event.results[0][0].transcript;
        appendMsg('user', transcript);
        recognizing = false;
        try { recognition.stop(); } catch(e) {}
        setAsrStatus('idle');
        sendToAgent(transcript);
      }
    };

    recognition.onerror = function(event) {
      setAsrStatus('error: ' + event.error);
      recognizing = false;
    };

    recognition.onend = function() {
      recognizing = false;
      setAsrStatus('idle');
      if (ambientToggle.checked && asrModeSel.value === 'browser') {
        try { recognition.start(); recognizing = true; setAsrStatus('ambient listening…'); } catch(e) {}
      }
    };
  }

  async function startPtt() {
    if (asrModeSel.value === 'server') {
      serverAsr.startSession('ptt').catch(e => appendMeta('ASR start failed: ' + e));
      return;
    }
    if (!recognition) {
      alert('Web Speech API not supported. Use Server (AWS Transcribe) mode.');
      return;
    }
    recognition.lang = languageSel.value || 'en-US';
    try { recognition.start(); recognizing = true; setAsrStatus('listening (browser)…'); } catch(e) {}
  }

  function stopPtt() {
    if (asrModeSel.value === 'server') {
      serverAsr.stopSession();
      return;
    }
    if (recognition && recognizing) {
      try { recognition.stop(); } catch(e) {}
      recognizing = false;
      setAsrStatus('idle');
    }
  }

  async function loadDebugTools() {
    if (!debugToolsContainer) return;
    try {
      const resp = await fetch('/api/debug/tools');
      const data = await resp.json();
      renderDebugTools(data.tools || []);
      (data.logs || []).forEach((l) => appendDebugLog(l));
    } catch (e) {
      appendDebugLog('Failed to load debug tools: ' + e);
    }
  }

  function renderDebugTools(tools) {
    debugToolsContainer.innerHTML = '';
    tools.forEach((tool) => {
      const wrapper = document.createElement('div');
      wrapper.className = 'debug-tool';

      const title = document.createElement('div');
      title.className = 'debug-tool-title';
      title.textContent = tool.label || tool.id;
      wrapper.appendChild(title);

      if (tool.description) {
        const desc = document.createElement('div');
        desc.className = 'meta';
        desc.textContent = tool.description;
        wrapper.appendChild(desc);
      }

      let inputEl = null;
      if (tool.param_label) {
        inputEl = document.createElement('input');
        inputEl.type = 'text';
        inputEl.placeholder = tool.param_label;
        if (tool.default_param) inputEl.value = tool.default_param;
        inputEl.style.marginTop = '6px';
        inputEl.style.marginRight = '6px';
        wrapper.appendChild(inputEl);
      }

      const runBtn = document.createElement('button');
      runBtn.textContent = 'Run';
      runBtn.onclick = () => {
        const payload = inputEl ? { [tool.param_key || 'input']: (inputEl.value || '') } : {};
        runDebugTool(tool.id, payload);
      };
      wrapper.appendChild(runBtn);

      debugToolsContainer.appendChild(wrapper);
    });
  }

  async function runDebugTool(toolId, params) {
    setDebugOutput('Running ' + toolId + '...');
    try {
      const resp = await fetch('/api/debug/run_tool', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ tool: toolId, params: params || {} })
      });
      const data = await resp.json();
      const prefix = data.ok ? '[ok]' : '[error]';
      setDebugOutput(prefix + '\n' + (data.output || ''));
      if (data.logs) {
        (data.logs || []).forEach((l) => appendDebugLog(l));
      }
    } catch (e) {
      setDebugOutput('Failed to run tool: ' + e);
    }
  }

  async function refreshVerboseLogs() {
    if (!verboseEnabled || !debugLogBox) return;
    try {
      const resp = await fetch('/api/debug/logs');
      const data = await resp.json();
      debugLogBox.innerHTML = '';
      (data.lines || []).forEach((l) => appendDebugLog(l));
    } catch (e) {
      // best-effort only
    }
  }

  // PTT button: hold to talk
  pttBtn.addEventListener('mousedown', startPtt);
  pttBtn.addEventListener('touchstart', (e) => { e.preventDefault(); startPtt(); }, {passive:false});
  window.addEventListener('mouseup', stopPtt);
  window.addEventListener('touchend', stopPtt);

  // Ambient toggle behavior
  ambientToggle.addEventListener('change', () => {
    const on = ambientToggle.checked;
    if (asrModeSel.value === 'server') {
      if (on) {
        serverAsr.startCapture().then(() => setAsrStatus('ambient armed')).catch(e => appendMeta('capture failed: ' + e));
      } else {
        serverAsr.stopSession();
        serverAsr.stopCapture();
        setAsrStatus('idle');
      }
    } else {
      if (on && recognition) {
        try { recognition.lang = languageSel.value || 'en-US'; recognition.start(); recognizing = true; setAsrStatus('ambient listening…'); } catch(e) {}
      }
    }
  });

  // ASR mode change: stop current sessions.
  asrModeSel.addEventListener('change', () => {
    setPartial('');
    stopPtt();
    serverAsr.stopSession();
    serverAsr.stopCapture();
    setAsrStatus('idle');
  });

  // Initialize device lists.
  (async () => {
    try {
      await refreshDevices();
    } catch (e) {
      appendMeta('device enumeration blocked (click “Enable audio”): ' + e);
    }
    try {
      await loadDebugTools();
      await refreshVerboseLogs();
      if (verboseEnabled) {
        setInterval(refreshVerboseLogs, 10000);
      }
    } catch (e) {
      appendDebugLog('debug init failed: ' + e);
    }
  })();
</script>
</body>
</html>
